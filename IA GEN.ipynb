{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IA Generative Big Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creative Writing, Q&A, Text Summarization, Data Extraction,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed the model via the prompt the following elements :\n",
    "\n",
    "the instructions, \n",
    "\n",
    "external information / contexts (We do it manually, Web Search API), \n",
    "\n",
    "user input, \n",
    "\n",
    "output indicator (The start of what we would like the model to begin generating),\n",
    "\n",
    "... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with \"LangChain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain\n",
    "# ! pip install langchain_community\n",
    "# ! pip install langchain_ollama\n",
    "# ! pip install langchain_experimental\n",
    "# # ! pip install -U langchain-openai langchainhub\n",
    "# ! pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools = [PythonREPLTool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub, PromptTemplate\n",
    "from langchain.agents import create_openai_functions_agent, create_react_agent, AgentExecutor\n",
    "from langchain_community.chat_models import ChatAnthropic\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_experimental.tools import PythonREPLTool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided answer with \"I don't know\". \n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP. \n",
    "Their superior performance over smaller models has made them incredibly useful for developers building NLP enabled applications. \n",
    "These models can be accessed via Hugging Face's 'transformers' library, via OpenAI using the 'openai' library, and via Cohere using the 'cohere' library. \n",
    "\n",
    "Question: Which libraries and model providers offer LLMs? \n",
    "\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, so I need to figure out who Elon Musk's father is based on the information provided. Let me start by recalling what I know about Elon Musk.\n",
      "\n",
      "Elon Musk is a co-founder of Twitter and Tesla. He's been involved in many tech companies like SpaceX and Meta. Now, he mentioned that his father was a developer or someone interested in NLP. \n",
      "\n",
      "Looking at the context given, it's about Large Language Models (LLMs) being used in NLP, their access points via Hugging Face, OpenAI, or Cohere libraries. However, the question is specifically about Elon Musk's father's identity, which isn't directly related to his work as a developer or technology investor.\n",
      "\n",
      "The context doesn't provide any information about Elon Musk's family background. It only discusses his professional life and the tools he uses in NLP development. So, without additional context on his family members' roles or contributions, I can't determine who his father is based on this information.\n",
      "\n",
      "Therefore, I should state that I don't know because there isn't enough information provided about Elon Musk's family to answer the question.\n",
      "</think>\n",
      "\n",
      "I don't know\n"
     ]
    }
   ],
   "source": [
    "# instructions = \"\"\"You are an agent designed to write and execute python code to answer questions.\n",
    "# You have access to a python REPL, which you can use to execute python code.\n",
    "# If you get an error, debug your code and try again.\n",
    "# Only use the output of your code to answer the question. \n",
    "# You might know the answer without running any code, but you should still run the code to get the answer.\n",
    "# If it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n",
    "# \"\"\"\n",
    "\n",
    "prompt = \"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided answer with \"I don't know\". \n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP. \n",
    "Their superior performance over smaller models has made them incredibly useful for developers building NLP enabled applications. \n",
    "These models can be accessed via Hugging Face's 'transformers' library, via OpenAI using the 'openai' library, and via Cohere using the 'cohere' library. \n",
    "\n",
    "Question: who is elon musk's father?\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt)\n",
    "model = OllamaLLM(model='deepseek-r1:1.5b') #\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\"qestion\":\" Which one of the LLMs is the most efficient ?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2756\\3701019134.py:27: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = model(prompt_template.format(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out which libraries and model providers offer large language models (LLMs). The context provided mentions that LLMs are a type of NLP model. It says that these models can be accessed via Hugging Face's 'transformers' library, OpenAI using the 'openai' library, and Cohere using the 'cohere' library. \n",
      "\n",
      "So, looking at that, I think each of those libraries is specifically for LLMs because they're all mentioned in relation to them. Hugging Face is a big deal in NLP, especially with Transformers. OpenAI's transformers are their core technology. And Cohere is another company known for AI tools and models. \n",
      "\n",
      "The question is asking which libraries and model providers offer LLMs. So I think the answer should list all these specific libraries because they're each dedicated to providing LLMs. There's no other information given about other libraries or model providers in that context, so I can't find more.\n",
      "</think>\n",
      "\n",
      "The libraries mentioned for accessing Large Language Models (LLMs) are Hugging Face's 'transformers' library, OpenAI's 'openai' library, and Cohere's 'cohere' library. These are the primary providers within their respective categories.\n",
      "\n",
      "Answer: The libraries are Hugging Face's 'transformers', OpenAI's 'openai', and Cohere's 'cohere'.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided answer with \"I don't know\". \n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP. \n",
    "Their superior performance over smaller models has made them incredibly useful for developers building NLP enabled applications. \n",
    "These models can be accessed via Hugging Face's 'transformers' library, via OpenAI using the 'openai' library, and via Cohere using the 'cohere' library. \n",
    "\n",
    "Question: {query}?\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = OllamaLLM(model='deepseek-r1:1.5b') #\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables = [\"query\"],\n",
    "    template = template\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "response = model(prompt_template.format(\n",
    "    query=\"Which libraries and model providers offer LLMs?\"\n",
    "))\n",
    "        \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"Docs/A Survey on Diffusion Models for Anomaly Detection.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "# documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** models are an innovative approach in the field of artificial intelligence, aiming to combine the power of text generation models with the efficiency of search engines. Unlike traditional text generation models that rely solely on knowledge acquired during training, RAGs integrate a real-time information retrieval phase from an external database or a corpus of documents.\n",
    "\n",
    "This architecture is based on two main steps:\n",
    "1. **Retrieval**: The model searches a large database or a set of relevant documents based on the user's query.\n",
    "2. **Generation**: The model uses the retrieved information to generate a more precise and informed answer, combining both the pre-existing context and the new data retrieved.\n",
    "\n",
    "RAGs are particularly useful in contexts where information is changing rapidly or where the knowledge base is too large to be fully included in the model. For example, in chatbots or question-answering (QA) systems, this approach can improve the relevance of answers and reduce bias or errors related to missing or outdated information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
