{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IA Generative Big Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creative Writing, Q&A, Text Summarization, Data Extraction,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed the model via the prompt the following elements :\n",
    "\n",
    "the instructions, \n",
    "\n",
    "external information / contexts (We do it manually, Web Search API), \n",
    "\n",
    "user input, \n",
    "\n",
    "output indicator (The start of what we would like the model to begin generating),\n",
    "\n",
    "... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install openai\n",
    "# ! pip install llama-cpp-python\n",
    "# ! pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided answer with \"I don't know\". \n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP. \n",
    "Their superior performance over smaller models has made them incredibly useful for developers building NLP enabled applications. \n",
    "These models can be accessed via Hugging Face's 'transformers' library, via OpenAI using the 'openai' library, and via Cohere using the 'cohere' library. \n",
    "\n",
    "Question: Which libraries and model providers offer LLMs? \n",
    "\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "\n",
    "\n",
    "# # openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = openai.completions.create(\n",
    "#     model=\"o1-preview\",  # Spécifiez le modèle\n",
    "#     max_tokens=256,\n",
    "#     temperature = 0,\n",
    "#     prompt=prompt\n",
    "# )\n",
    "# print(response['choices'][0]['texts'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = openai.models.list()\n",
    "# # available_models = [model[\"id\"] for model in models[\"data\"]]\n",
    "# # available_models\n",
    "# models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain\n",
    "# ! pip install langchain_community\n",
    "# ! pip install langchain_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms import Ollama\n",
    "# from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "# # Initialize Ollama with your chosen model\n",
    "# # llm = Ollama(model=\"llama3.2\")\n",
    "# llm = ChatOllama(\n",
    "#     model=\"llama3.1\",\n",
    "#     temperature=0,\n",
    "#     # other params...\n",
    "# )\n",
    "\n",
    "\n",
    "# # Invoke the model with a query\n",
    "# response = llm.invoke(\"What is LLM?\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ollama\n",
    "\n",
    "# # ollama.pull('mistral')\n",
    "\n",
    "# response = ollama.chat(\"mistral\", messages=[{\n",
    "#         'role': 'user',\n",
    "#         'content': prompt,}])\n",
    "\n",
    "# response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ollama.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain\n",
    "# ! pip install -U langchain-openai langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain import hub\n",
    "# from langchain.agents import AgentExecutor\n",
    "# from langchain_experimental.tools import PythonREPLTool\n",
    "# from langchain.agents import create_react_agent\n",
    "# from langchain_community.chat_models import ChatAnthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools = [PythonREPLTool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, I need to figure out who Elon Musk's father is based on the context provided. Let me read through the context again.\n",
      "\n",
      "The context mentions that large Language Models (LLMs) are used in NLP and are available through various libraries: Hugging Face's transformers, OpenAI's 'openai' library, and Cohere's 'cohere' library. It also doesn't directly provide information about Elon Musk or his family members.\n",
      "\n",
      "Now, I'm trying to think if there's any mention of Elon Musk in the context about his family or parents. I don't see a single sentence that mentions anyone as Elon's father. All the information provided is related to AI models and their access via different libraries.\n",
      "\n",
      "Since there's no direct information about Elon Musk's family members, I should consider whether there might be some assumptions here. Maybe in the original context from which this question was derived, there were additional details or perhaps an image that included Elon's family? Without more context, it's not possible to determine his father's name.\n",
      "\n",
      "I also wonder if this is a hypothetical scenario or part of a joke. The user provided the initial context and then asked about Elon Musk's father based on it. Since the given context doesn't contain any information about Elon's family, I have to conclude that I don't know who his father is in this case.\n",
      "</think>\n",
      "\n",
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "# instructions = \"\"\"You are an agent designed to write and execute python code to answer questions.\n",
    "# You have access to a python REPL, which you can use to execute python code.\n",
    "# If you get an error, debug your code and try again.\n",
    "# Only use the output of your code to answer the question. \n",
    "# You might know the answer without running any code, but you should still run the code to get the answer.\n",
    "# If it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n",
    "# \"\"\"\n",
    "\n",
    "prompt = \"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided answer with \"I don't know\". \n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP. \n",
    "Their superior performance over smaller models has made them incredibly useful for developers building NLP enabled applications. \n",
    "These models can be accessed via Hugging Face's 'transformers' library, via OpenAI using the 'openai' library, and via Cohere using the 'cohere' library. \n",
    "\n",
    "Question: who is elon musk's father?\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt)\n",
    "model = OllamaLLM(model='deepseek-r1:1.5b') #\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\"qestion\":\" Which one of the LLMs is the most efficient ?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2756\\3701019134.py:27: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = model(prompt_template.format(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out which libraries and model providers offer large language models (LLMs). The context provided mentions that LLMs are a type of NLP model. It says that these models can be accessed via Hugging Face's 'transformers' library, OpenAI using the 'openai' library, and Cohere using the 'cohere' library. \n",
      "\n",
      "So, looking at that, I think each of those libraries is specifically for LLMs because they're all mentioned in relation to them. Hugging Face is a big deal in NLP, especially with Transformers. OpenAI's transformers are their core technology. And Cohere is another company known for AI tools and models. \n",
      "\n",
      "The question is asking which libraries and model providers offer LLMs. So I think the answer should list all these specific libraries because they're each dedicated to providing LLMs. There's no other information given about other libraries or model providers in that context, so I can't find more.\n",
      "</think>\n",
      "\n",
      "The libraries mentioned for accessing Large Language Models (LLMs) are Hugging Face's 'transformers' library, OpenAI's 'openai' library, and Cohere's 'cohere' library. These are the primary providers within their respective categories.\n",
      "\n",
      "Answer: The libraries are Hugging Face's 'transformers', OpenAI's 'openai', and Cohere's 'cohere'.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided answer with \"I don't know\". \n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP. \n",
    "Their superior performance over smaller models has made them incredibly useful for developers building NLP enabled applications. \n",
    "These models can be accessed via Hugging Face's 'transformers' library, via OpenAI using the 'openai' library, and via Cohere using the 'cohere' library. \n",
    "\n",
    "Question: {query}?\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = OllamaLLM(model='deepseek-r1:1.5b') #\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables = [\"query\"],\n",
    "    template = template\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "\n",
    "response = model(prompt_template.format(\n",
    "    query=\"Which libraries and model providers offer LLMs?\"\n",
    "))            \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
