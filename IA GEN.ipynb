{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IA Generative Big Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creative Writing, Q&A, Text Summarization, Data Extraction,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed the model via the prompt the following elements :\n",
    "\n",
    "the instructions, \n",
    "\n",
    "external information / contexts (We do it manually, Web Search API), \n",
    "\n",
    "user input, \n",
    "\n",
    "output indicator (The start of what we would like the model to begin generating),\n",
    "\n",
    "... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with \"LangChain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain\n",
    "# ! pip install langchain_community\n",
    "# ! pip install langchain_ollama\n",
    "# ! pip install langchain_experimental\n",
    "# # ! pip install -U langchain-openai langchainhub\n",
    "# ! pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools = [PythonREPLTool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub, PromptTemplate\n",
    "from langchain.agents import create_openai_functions_agent, create_react_agent, AgentExecutor\n",
    "from langchain_community.chat_models import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_experimental.tools import PythonREPLTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided answer with \"I don't know\". \n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP. \n",
    "Their superior performance over smaller models has made them incredibly useful for developers building NLP enabled applications. \n",
    "These models can be accessed via Hugging Face's 'transformers' library, via OpenAI using the 'openai' library, and via Cohere using the 'cohere' library. \n",
    "\n",
    "Question: Which libraries and model providers offer LLMs? \n",
    "\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out who Elon Musk's father is based on the given context. Let me read through the context again carefully.\n",
      "\n",
      "The context says that Large Language Models (LLMs) are the latest models used in NLP. It mentions that their superior performance has made them incredibly useful for developers building NLP applications. Then it provides three ways these models can be accessed: via Hugging Face's 'transformers' library, OpenAI using the 'openai' library, and Cohere with the 'cohere' library.\n",
      "\n",
      "Hmm, so where does Elon Musk come in here? He is a well-known figure in the tech industry, particularly known for his ventures like Twitter, SpaceX, SpaceX rocket launches, etc. I remember he's been involved in AI projects as well, maybe in the development of LLMs or other NLP models.\n",
      "\n",
      "Wait, let me think about him more specifically. Elon Musk founded Twitter and was part of the early days of Starship, which successfully launched in 2016. He also worked on several startups that aimed towards AI technology, including Hugging Face, which is mentioned in the context for accessing LLMs. So, Hugging Face's name is associated with Elon Musk.\n",
      "\n",
      "But does that mean his father? I don't recall any public information linking Elon Musk directly to a notable figure in history by his father. He was born on February 12, 1972, and the context doesn't mention anything about his family or ancestors.\n",
      "\n",
      "Therefore, without additional information from the provided context, it's not possible to determine who Elon Musk's father is based solely on this information.\n",
      "</think>\n",
      "\n",
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# instructions = \"\"\"You are an agent designed to write and execute python code to answer questions.\n",
    "# You have access to a python REPL, which you can use to execute python code.\n",
    "# If you get an error, debug your code and try again.\n",
    "# Only use the output of your code to answer the question. \n",
    "# You might know the answer without running any code, but you should still run the code to get the answer.\n",
    "# If it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n",
    "# \"\"\"\n",
    "\n",
    "prompt = \"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided answer with \"I don't know\". \n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP. \n",
    "Their superior performance over smaller models has made them incredibly useful for developers building NLP enabled applications. \n",
    "These models can be accessed via Hugging Face's 'transformers' library, via OpenAI using the 'openai' library, and via Cohere using the 'cohere' library. \n",
    "\n",
    "Question: who is elon musk's father?\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt)\n",
    "model = OllamaLLM(model='deepseek-r1:1.5b') #\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\"qestion\":\" Which one of the LLMs is the most efficient ?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2756\\3701019134.py:27: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = model(prompt_template.format(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out which libraries and model providers offer large language models (LLMs). The context provided mentions that LLMs are a type of NLP model. It says that these models can be accessed via Hugging Face's 'transformers' library, OpenAI using the 'openai' library, and Cohere using the 'cohere' library. \n",
      "\n",
      "So, looking at that, I think each of those libraries is specifically for LLMs because they're all mentioned in relation to them. Hugging Face is a big deal in NLP, especially with Transformers. OpenAI's transformers are their core technology. And Cohere is another company known for AI tools and models. \n",
      "\n",
      "The question is asking which libraries and model providers offer LLMs. So I think the answer should list all these specific libraries because they're each dedicated to providing LLMs. There's no other information given about other libraries or model providers in that context, so I can't find more.\n",
      "</think>\n",
      "\n",
      "The libraries mentioned for accessing Large Language Models (LLMs) are Hugging Face's 'transformers' library, OpenAI's 'openai' library, and Cohere's 'cohere' library. These are the primary providers within their respective categories.\n",
      "\n",
      "Answer: The libraries are Hugging Face's 'transformers', OpenAI's 'openai', and Cohere's 'cohere'.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided answer with \"I don't know\". \n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP. \n",
    "Their superior performance over smaller models has made them incredibly useful for developers building NLP enabled applications. \n",
    "These models can be accessed via Hugging Face's 'transformers' library, via OpenAI using the 'openai' library, and via Cohere using the 'cohere' library. \n",
    "\n",
    "Question: {query}?\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = OllamaLLM(model='deepseek-r1:1.5b') #\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables = [\"query\"],\n",
    "    template = template\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "response = model(prompt_template.format(\n",
    "    query=\"Which libraries and model providers offer LLMs?\"\n",
    "))\n",
    "        \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** models are an innovative approach in the field of artificial intelligence, aiming to combine the power of text generation models with the efficiency of search engines. Unlike traditional text generation models that rely solely on knowledge acquired during training, RAGs integrate a real-time information retrieval phase from an external database or a corpus of documents.\n",
    "\n",
    "This architecture is based on two main steps:\n",
    "1. **Retrieval**: The model searches a large database or a set of relevant documents based on the user's query.\n",
    "2. **Generation**: The model uses the retrieved information to generate a more precise and informed answer, combining both the pre-existing context and the new data retrieved.\n",
    "\n",
    "RAGs are particularly useful in contexts where information is changing rapidly or where the knowledge base is too large to be fully included in the model. For example, in chatbots or question-answering (QA) systems, this approach can improve the relevance of answers and reduce bias or errors related to missing or outdated information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type 1 : Standard RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_path = \"Docs/A Survey on Diffusion Models for Anomaly Detection.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "# documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using Hugging Face Transformers ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27459710cdbf45bd8af5b6c4e235d412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  17%|#6        | 73.4M/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ccff7c2115f488a931207f5d5df1859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11bd5765f8f8404b841e1cd60eda27c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5557925f35204d6da89e39c7b8606b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab932669e2ea4ecd867813b0bec22a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c0b30f03d04b0baf18ee7f857eda3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-mpnet-base-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ! pip install sentence_transformers \n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Hugging Face Transformers\n",
    "# Uses models from the Hugging Face library.\n",
    "# Ideal for leveraging a wide variety of models for different tasks.\n",
    "# Note: Running Hugging Face models locally on your machine incurs no direct cost other than using your computational resources.\n",
    "# Note: Find other models at https://huggingface.co/models?other=embeddings\n",
    "print(\"\\n--- Using Hugging Face Transformers ---\")\n",
    "huggingface_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "huggingface_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\users\\user\\anaconda3\\lib\\site-packages (1.41.1)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (4.2.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (1.5.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (10.2.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (17.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (13.3.5)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (8.2.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (2.1.6)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (3.1.37)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (6.3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
      "Requirement already satisfied: toolz in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2023.3.post1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas<3,>=1.4.0->streamlit) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type 2 : Graph RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type 3 : Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
